---
title: "AMS 572 Final Project"
author: "Branden Ciranni & Ashler Herrick"
date: "December 01, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analysis of Diabetes Data

```{r message=FALSE, warning=FALSE}
# Library Requirements - Separate file to keep things neat.
source("libraries.R")

# Utility Functions, in separate script for neatness
source("utilities.R")

data <- read.csv("./data/diabetes.csv", header = TRUE)
head(data)
```

```{r}
unique(data$location)
```

We add an additonal field to this data for classification purposes. It is taken as a diagnosis of diabetes if the "Glycosated Hemoglobin" measurement `glyhb >= 7.0`. This will be assumed to be the case for this analysis.

We also add the ratio `waist_hip_ratio` = $\frac{waist}{hip}$.

```{r}
data <- transform(data, has_diabetes = glyhb >= 7.0, wh_ratio = waist/hip)
```

```{r}
data %>% group_by(has_diabetes) %>%
          count()
```

### Diabetes vs Not Diabetes

```{r}
ggplot(data, aes(x=has_diabetes)) + 
  geom_bar(aes(fill=has_diabetes))
```

We observe that we have an unbalanced dataset, with far more study participants that are testing negative for diabetes, than positive. We will drop the rows which have null values for the diabetes classification.

### Analysis of Null Values

```{r}
# First, mutate dataframe to change empty strings to NA
data <- data %>% mutate_if(is.character, list(~na_if(.,""))) 

# Then, use the count_null function in our utilities script
count_null(data)
```

We observe that there are a large number of null values in the `bp.2s` and `bp.2d` columns. We otherwise see a very low number of null values. We can discard these two columns given that $65\%$ of the data does not have values.

```{r}
data <- data %>% dplyr::select(-c('bp.2s', 'bp.2d'))
```

Because Our Analysis Will focus on the indicators of diabetes, we will exclude those rows which have null values for `glyhb`, and thus, our indicator variable `has_diabetes`.

```{r}
data <- filter(data, !is.na(has_diabetes))
```

### Plot all Numeric Variables for Data Exploration

#### Box Plots

```{r message=FALSE, warning=FALSE}
data %>%
  dplyr::select(-c("id", "location", "gender", "frame")) %>%
  gather(key="feature", value="value", -has_diabetes) %>% 
  ggplot(aes(x=feature, y=value, fill=has_diabetes)) +
    facet_wrap(~ feature, scales = "free") +
    geom_boxplot()
```

#### Histogram / Density Plots

```{r message=FALSE, warning=FALSE}
data %>%
  dplyr::select(-c("id", "location", "gender", "frame")) %>%
  gather(key="feature", value="value", -has_diabetes) %>% 
  ggplot(aes(value)) +
    facet_wrap(~ feature, scales = "free") +
    geom_histogram(aes(y=..density.., fill=has_diabetes)) +
    geom_density(aes(y=..density.., fill=has_diabetes, alpha=0.1))
```

We see that some features in our data are clearly not normally distributed. We will need to take this into account in future analyses. However, there do appear to be several natural hypotheses to test. Variables that have a significant shift in the center of the distribution are:   
  - `age`   
  - `stab.glu`   
  - `waist`   
  - `weight`   
  - `wh_ratio`   
  
Because we are not doing any sort of analysis on `glyhb` and are rather using `glyhb > 7.0` as an indicator variable, we do not have to do any sort of transform, but a log or sqrt transform could make it behave more normally.

Map categorical value in `frame` to ordinal data type. Levels are as follows: `r unique(data$frame)`.
We provide a utility method `cat_to_ord` in `utilities.R` which maps this ordered list to increasing integer values.

```{r}
# Use our Utility function in `utilities.R` to map the categorical, but ordinal string values to
# numerical integer values so we may include them in the correlation analysis
data$frame <- cat_to_ord(data, "frame", c("small", "medium", "large"))
```

The new Unique values are shown here:

```{r}
unique(data$frame)
```

### Waist Hip Ratio, segmented by Diabetes Diagnosis
Analyze the effect of `wh_ratio` on Diabetes Diagnosis.

```{r}
ggplot(data, aes(x=has_diabetes, y=wh_ratio)) +
  geom_violin(aes(fill=factor(has_diabetes))) +
  geom_boxplot(width=0.1)
```

The violin & box plot suggest that the mean waist hip ratio is likely to be greater in participants with diabetes.

```{r}
wh_data <- data %>%
            filter(!is.na(wh_ratio)) %>%
            group_by(has_diabetes) %>%
            summarize(mean = mean(wh_ratio),
                      var = var(wh_ratio),
                      n = n())

wh_data
```

Use a T-test to test whether there is a significant difference between the means, and an F-test to test the equal variance assumption.

First, the data is prepared.

```{r}
diabetes_pos <- data %>% filter(has_diabetes)
diabetes_neg <- data %>% filter(!has_diabetes)

wh_pos <- diabetes_pos$wh_ratio
wh_neg <- diabetes_neg$wh_ratio
```

#### Test for Normality

Testing the data corresponding to positive diabetes test (`has_diabetes = TRUE`) at $\alpha = 0.05$.

```{r}

shapiro.test(wh_pos)
qqnorm(wh_pos, main="Waist/Hip Ratio (Diabetes Positive)")
qqline(wh_pos, col='red')
```

We see $p > 0.05$, thus we cannot reject the normality assumption.

Testing the data corresponding to negative diabetes test. (`has_diabetes = FALSE`) at $\alpha = 0.05$.

```{r}
shapiro.test(wh_neg)
qqnorm(wh_neg, main="Waist/Hip Ratio (Diabetes Negative)")
qqline(wh_neg, col='red')
```

Likewise, we see $p > 0.05$, and cannot reject the normality assumption. By the above Shapiro-Wilk Tests, we can assume normality in the data with $95\%$ confidence. 

First, we test the equal variance assumption with an F Test.

```{r}
var.test(wh_pos, wh_neg, alternative = "two.sided")
```

Since $p > 0.05$, we can assume equal variances. Then the t-test gives us:

```{r}
t.test(wh_pos, wh_neg, var.equal = TRUE)
```

Thus, since $p < 0.05$, there is a statistically significant difference between the waist hip ratio in participants with diabetes and without diabetes.

### Calculate Kendall's Correlation Coefficient between all variables
We see above in the data exploration that some variables have a very large variance, while others have a much tighter distribution. The best example of this is stabilized glucose, `stab.glu`, which shows a large number of outliers in participants without diabetes.

Additionally, the data is not guaranteed to be normal.

As Kendall's coefficient is less sensitive to outliers and does not require normality or equality of variance, we use it over Pearson's.


```{r}
ggcorr(data, method = c("pairwise", "kendall"))
```

We see the strongest correlations between obviously correlated features such as waist, weight, and hip. We also see a moderate correlation between the Stabilized Glucose Level and Glycosated Hemoglobin Level, our indicator for diabetes. We also notice a somewhat positive correlation to `glyhb` for other factors above.

Let us examine some interesting features more closely, colored by diabetes diagnosis:

```{r message=FALSE, warning=FALSE}
ggpairs(data, 
        columns = c("glyhb", "waist", "wh_ratio", "stab.glu"), 
        title = "Pairwise Analysis of Diabetes Features", 
        upper = list(continuous = wrap("cor", method = 'kendall', size = 3)),
        lower = list(continuous = wrap("smooth", alpha = 0.5, size = 0.3)),
        mapping = aes(color = has_diabetes, alpha=0.5))
```

##Stepwise Logistic Regression to find the best predictors
```{r}
data$weight_height_ratio <- with(data, weight/height)
set.seed(123)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
no.na.train <- na.omit(train)
no.na.test <- na.omit(test)
logistic_model <- glm(has_diabetes ~ stab.glu + chol + hdl + weight_height_ratio + wh_ratio + age,data = no.na.train, family = binomial) %>%
  stepAIC(trace = TRUE, direction = 'backward')
summary(logistic_model)
```
We can calculate the variance inflation factor, or VIF, to measure the multicollinearity of each predictor. VIF is bounded below by one and unbounded above.
```{r}
vif(logistic_model)

```
The stepwise logistic regression with the missing values removed finds that the best model is has the predictors stable glucose, cholesterol, weight/height ratio and age.
```{r}
exp(coef(logistic_model))
probabilities <- logistic_model %>% predict(no.na.test, type = "response")
head(probabilities)
```
```{r}
predicted.classes <- ifelse(probabilities > 0.10, TRUE, FALSE)
head(predicted.classes)
```
```{r}
mean(predicted.classes == no.na.test$has_diabetes)
mean(predicted.classes == no.na.test$has_diabetes & no.na.test$has_diabetes == TRUE)/mean(no.na.test$has_diabetes == TRUE)
```
The first number is the overall classification rate of the model, and the second number is the observed power of the classifier i.e. the proportion of case in which the model identified diabetes, given diabetes was present. The probability of a type two error, or false negative, is one minus the power of the test.

```{r}
no.na.train %>%
  mutate(prob = ifelse(has_diabetes == TRUE,1,0)) %>%
  ggplot(aes(stab.glu,prob)) +
  geom_point(alpha =0.2) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  labs(
    title = "Logistic Regression Model",
    x = "Stable Glucose",
    y = "Estimated Probability of Diabetes"
  )

```

```{r}
no.na.train %>%
  mutate(prob = ifelse(has_diabetes == TRUE,1,0)) %>%
  ggplot(aes(chol,prob)) +
  geom_point(alpha =0.2) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  labs(
    title = "Logistic Regression Model",
    x = "Cholesterol",
    y = "Estimated Probability of Diabetes"
  )
```

```{r}
no.na.train %>%
  mutate(prob = ifelse(has_diabetes == TRUE,1,0)) %>%
  ggplot(aes(age,prob)) +
  geom_point(alpha =0.2) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  labs(
    title = "Logistic Regression Model",
    x = "Age",
    y = "Estimated Probability of Diabetes"
  )
```

```{r}
no.na.train %>%
  mutate(prob = ifelse(has_diabetes == TRUE,1,0)) %>%
  ggplot(aes(weight_height_ratio,prob)) +
  geom_point(alpha =0.2) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  labs(
    title = "Logistic Regression Model",
    x = "Weight/Height Ratio",
    y = "Estimated Probability of Diabetes"
  )
```
The plots give the estimated probability of diabetes with respect to each variable.


##Effect of Missing values
If we fit the model to the original train data with 20% of the data removed, we obtain a slightly different model.
```{r}
set.seed(123)
dt = sort(sample(nrow(no.na.train), nrow(no.na.train)*.8))
missing_train<-data[dt,]
logistic_model_missing <- glm(has_diabetes ~ stab.glu + chol + age + weight_height_ratio, data = missing_train, family = binomial)
summary(logistic_model_missing)
```
If we randomly remove the data we see that the fit is improved. This makes sense because there is less variation, and the model fit can be better fit to the data. However, this does not actually mean that the model is better, only that it is a better fit for the data. 
```{r}
probabilities_missing <- logistic_model_missing %>% predict(no.na.test, type = "response")
head(probabilities_missing)
```
```{r}
predicted.classes.missing <- ifelse(probabilities_missing > 0.10, TRUE, FALSE)
mean(predicted.classes.missing == no.na.test$has_diabetes)
mean(predicted.classes.missing == no.na.test$has_diabetes & no.na.test$has_diabetes == TRUE)/mean(no.na.test$has_diabetes == TRUE)
```
The tests were done on the same dataset. we can see that both the power and the overall classification rate are lower when we train the model on a smaller dataset, i.e. when we are dealing with missing values.